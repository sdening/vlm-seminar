{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "089a1368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add MedCLIP folder to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', 'MedCLIP'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cacd2118",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'medclip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmedclip\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(medclip\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__file__\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'medclip'"
     ]
    }
   ],
   "source": [
    "import medclip\n",
    "print(medclip.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d7d8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sandradening/anaconda3/envs/Sandra_Coding/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'medclip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmedclip\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_medclip\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MedCLIPVisionModelViT\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'medclip'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "from medclip.modeling_medclip import MedCLIPVisionModelViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e0549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = \"../MedCLIP/checkpoints/rsna_binary_classification_SU/final_model.bin\"\n",
    "csv_path = \"rsna_samples.csv\"  # CSV must contain columns: path, label\n",
    "max_samples = 300  # limit number to speed up embedding + plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d14631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Load MedCLIP Vision Model\n",
    "# -----------------------------\n",
    "model = MedCLIPVisionModelViT()\n",
    "state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "vision_state_dict = {\n",
    "    k.replace(\"vision_model.\", \"\"): v\n",
    "    for k, v in state_dict.items()\n",
    "    if k.startswith(\"vision_model.\")\n",
    "}\n",
    "model.load_state_dict(vision_state_dict)\n",
    "model.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1a25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Image preprocessing\n",
    "# -----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5862785803043838], std=[0.27950088968644304]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd6f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Load samples\n",
    "# -----------------------------\n",
    "df = pd.read_csv(csv_path).head(max_samples)\n",
    "\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    dicom_path = row[\"path\"]\n",
    "    label = int(row[\"label\"])\n",
    "\n",
    "    ds = pydicom.dcmread(dicom_path)\n",
    "    image_np = ds.pixel_array.astype(np.float32)\n",
    "    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
    "    image = Image.fromarray((image_np * 255).astype(np.uint8)).convert(\"L\").convert(\"RGB\")\n",
    "\n",
    "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = model(input_tensor, project=False)\n",
    "        emb = emb.cpu().squeeze().numpy()\n",
    "        embeddings.append(emb)\n",
    "        labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffbc240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Dimensionality reduction\n",
    "# -----------------------------\n",
    "print(\"Running t-SNE...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "emb_2d = tsne.fit_transform(np.array(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90477743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Plot\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "for lbl in [0, 1]:\n",
    "    idxs = np.where(np.array(labels) == lbl)[0]\n",
    "    plt.scatter(\n",
    "        emb_2d[idxs, 0], emb_2d[idxs, 1],\n",
    "        label=\"Pneumonia\" if lbl else \"Healthy\",\n",
    "        alpha=0.7, s=30\n",
    "    )\n",
    "\n",
    "plt.title(\"2D Embedding Visualization via t-SNE\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"embedding_plot.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
