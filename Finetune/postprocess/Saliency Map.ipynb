{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2cd38a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4543392b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f58e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_saliency_map(model, image, target_class=1):\n",
    "    model.eval()\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229]),  # For grayscale image, using single channel mean and std\n",
    "    ])\n",
    "    \n",
    "    # Convert the image to a tensor and set requires_grad to True\n",
    "    image_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "    image_tensor.requires_grad_()\n",
    "\n",
    "    # Simulate a batch: create the target label tensor for class 0 or 1\n",
    "    target_tensor = torch.tensor([target_class]).unsqueeze(0)  # Make it a batch of size 1\n",
    "\n",
    "    # Forward pass using the shared_step function\n",
    "    loss, logits, preds, _ = model.shared_step((image_tensor, target_tensor))  # Pass the image and target as a batch\n",
    "    \n",
    "    # Get the class score for the target class\n",
    "    class_score = logits[0, target_class]\n",
    "    \n",
    "    # Zero all previous gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute gradients with respect to the image\n",
    "    class_score.backward()\n",
    "    \n",
    "    # Get the gradient of the image\n",
    "    saliency, _ = torch.max(image_tensor.grad.data.abs(), dim=1)\n",
    "    \n",
    "    # Convert the saliency to numpy for visualization\n",
    "    saliency = saliency.squeeze().cpu().numpy()\n",
    "    \n",
    "    return saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "91eeb7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom_image(dicom_path):\n",
    "    # Read DICOM file\n",
    "    dicom_data = pydicom.dcmread(dicom_path)\n",
    "    \n",
    "    # Convert the pixel data to numpy array\n",
    "    image_array = dicom_data.pixel_array\n",
    "    \n",
    "    # If the image is grayscale (1 channel), convert to 3 channels (RGB)\n",
    "    if len(image_array.shape) == 2:  # Check if it's grayscale (height, width)\n",
    "        image_array = np.stack([image_array] * 3, axis=-1)  # Create 3 channels (RGB)\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    image = Image.fromarray(image_array)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4651064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_maps(image, saliency_class0, saliency_class1):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    # Plot the saliency maps for class 0 and class 1\n",
    "    ax[0].imshow(image, cmap='gray')\n",
    "    ax[0].imshow(saliency_class0, cmap='hot', alpha=0.5)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title(\"Attention for Class 0 (No Finding)\")\n",
    "\n",
    "    ax[1].imshow(image, cmap='gray')\n",
    "    ax[1].imshow(saliency_class1, cmap='hot', alpha=0.5)\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title(\"Attention for Class 1 (Abnormal)\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5d67a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration from: ../configs/rsna.yaml\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from train_cls import load_config\n",
    "from methods.cls_model import FinetuneClassifier\n",
    "from datasets.cls_dataset import RSNAImageClsDataset  # Using RSNA dataset now\n",
    "from datasets.data_module import DataModule\n",
    "from datasets.transforms import DataTransforms\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load config for RSNA dataset\n",
    "config = load_config('../configs/rsna.yaml')\n",
    "\n",
    "finetuned_rsna = FinetuneClassifier(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00f511c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (1) to match target batch_size (0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m load_dicom_image(dicom_path)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Generate saliency maps for class 0 and class 1\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m saliency_class0 \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_saliency_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinetuned_resnet50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m saliency_class1 \u001b[38;5;241m=\u001b[39m generate_saliency_map(finetuned_resnet50, image, target_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Plot the attention maps for both classes side by side\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 19\u001b[0m, in \u001b[0;36mgenerate_saliency_map\u001b[0;34m(model, image, target_class)\u001b[0m\n\u001b[1;32m     16\u001b[0m target_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([target_class])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Make it a batch of size 1\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Forward pass using the shared_step function\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m loss, logits, preds, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshared_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass the image and target as a batch\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get the class score for the target class\u001b[39;00m\n\u001b[1;32m     22\u001b[0m class_score \u001b[38;5;241m=\u001b[39m logits[\u001b[38;5;241m0\u001b[39m, target_class]\n",
      "File \u001b[0;32m~/Documents/Dokumente_Sandra/Master_Studium/3_Semester/VLM_Seminar/Code/VLP-Seminar/Finetune/methods/cls_model.py:110\u001b[0m, in \u001b[0;36mFinetuneClassifier.shared_step\u001b[0;34m(self, batch, return_embeddings)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    109\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m--> 110\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m     preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss, logits, preds, y\n",
      "File \u001b[0;32m~/anaconda3/envs/CAMP/lib/python3.10/site-packages/torch/nn/functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (1) to match target batch_size (0)."
     ]
    }
   ],
   "source": [
    "checkpoint_path = '/Users/sandradening/Documents/Dokumente_Sandra/Master_Studium/3_Semester/VLM_Seminar/Code/data/ckpts/FinetuneCLS/rsna/2025_01_17_14_27_25/epoch=27-step=8147.ckpt'\n",
    "\n",
    "\n",
    "finetuned_resnet50 = FinetuneClassifier(config)\n",
    "finetuned_resnet50.load_state_dict(torch.load(checkpoint_path, map_location=torch.device('cpu'))['state_dict'])\n",
    "\n",
    "first = \"/Users/sandradening/Documents/Dokumente_Sandra/Master_Studium/3_Semester/VLM_Seminar/Code/datasets/rsna/stage_2_train_images/f2698fda-0477-435f-b297-f1b284a731aa.dcm\"\n",
    "\n",
    "second = \"/Users/sandradening/Documents/Dokumente_Sandra/Master_Studium/3_Semester/VLM_Seminar/Code/datasets/rsna/stage_2_train_images/b76dd4b8-7b51-4cb6-8fd7-0b7365ef3e1e.dcm\"\n",
    "\n",
    "third = \"/Users/sandradening/Documents/Dokumente_Sandra/Master_Studium/3_Semester/VLM_Seminar/Code/datasets/rsna/stage_2_train_images/5d8dbcf9-0d68-4aec-8638-b0a9f45d71d6.dcm\"\n",
    "\n",
    "fourth = \"/Users/sandradening/Documents/Dokumente_Sandra/Master_Studium/3_Semester/VLM_Seminar/Code/datasets/rsna/stage_2_train_images/f6be6dc3-9539-46c0-a1f5-b10919ff81cd.dcm\"\n",
    "dicom_paths = [first, second, third, fourth]  # List your DICOM file paths\n",
    "\n",
    "\n",
    "for dicom_path in dicom_paths:\n",
    "    # Load and preprocess DICOM image\n",
    "    image = load_dicom_image(dicom_path)\n",
    "\n",
    "    # Generate saliency maps for class 0 and class 1\n",
    "    saliency_class0 = generate_saliency_map(finetuned_resnet50, image, target_class=0)\n",
    "    saliency_class1 = generate_saliency_map(finetuned_resnet50, image, target_class=1)\n",
    "\n",
    "    # Plot the attention maps for both classes side by side\n",
    "    plot_attention_maps(image, saliency_class0, saliency_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c0f00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d1d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a05dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c7d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f9575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4216662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "def generate_saliency_map(model, image, target_class=1):\n",
    "    \"\"\"\n",
    "    Generates the saliency map for a given class (0 or 1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    \n",
    "    # Convert the image to a tensor and set requires_grad to True\n",
    "    image_tensor = preprocess(image).unsqueeze(0)  # Add batch dimension\n",
    "    image_tensor.requires_grad_()\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(image_tensor)\n",
    "    \n",
    "    # Get the class score for the target class\n",
    "    class_score = output[0, target_class]\n",
    "    \n",
    "    # Zero all previous gradients\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass to compute gradients with respect to the image\n",
    "    class_score.backward()\n",
    "    \n",
    "    # Get the gradient of the image\n",
    "    saliency, _ = torch.max(image_tensor.grad.data.abs(), dim=1)\n",
    "    \n",
    "    # Convert the saliency to numpy for visualization\n",
    "    saliency = saliency.squeeze().cpu().numpy()\n",
    "    \n",
    "    return saliency\n",
    "\n",
    "def plot_attention_maps(image, saliency_class0, saliency_class1):\n",
    "    \"\"\"\n",
    "    Plot the attention maps (saliency maps) for both class 0 and class 1 side by side.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 7))\n",
    "\n",
    "    # Plot the saliency maps for class 0 and class 1\n",
    "    ax[0].imshow(image)\n",
    "    ax[0].imshow(saliency_class0, cmap='hot', alpha=0.5)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title(\"Attention for Class 0 (No Finding)\")\n",
    "\n",
    "    ax[1].imshow(image)\n",
    "    ax[1].imshow(saliency_class1, cmap='hot', alpha=0.5)\n",
    "    ax[1].axis('off')\n",
    "    ax[1].set_title(\"Attention for Class 1 (Abnormal)\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36244e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test image\n",
    "image_path = 'path_to_your_image.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Load the pre-trained ResNet50 model (or your fine-tuned model)\n",
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "# Generate saliency maps for class 0 and class 1\n",
    "saliency_class0 = generate_saliency_map(model, image, target_class=0)\n",
    "saliency_class1 = generate_saliency_map(model, image, target_class=1)\n",
    "\n",
    "# Plot the attention maps for both classes side by side\n",
    "plot_attention_maps(image, saliency_class0, saliency_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75548722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094bd666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa3cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in test_image_paths:\n",
    "    image = Image.open(image_path)\n",
    "    saliency_class0 = generate_saliency_map(finetuned_rsna, image, target_class=0)\n",
    "    saliency_class1 = generate_saliency_map(finetuned_rsna, image, target_class=1)\n",
    "    plot_attention_maps(image, saliency_class0, saliency_class1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
